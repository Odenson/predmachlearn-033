
R version 3.2.2 (2015-08-14) -- "Fire Safety"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> #!/usr/bin/R --verbose
> 
> # Notes
> 
> # Linear discriminant analysis lda MASS
> # http://www.statmethods.net/advstats/discriminant.html
> 
> # Running Script
> 
> # run this script from bash command line using
> # R --no-save < model-rf.R | tee data/run-lda.log
> 
> require(dplyr, quietly = TRUE)
> require(caret, quietly = TRUE)
> 
> # load raw data
> raw <- read.csv(
+     "data/pml-training.csv", header = TRUE,
+     na.strings = c("NA", "#DIV/0!"), stringsAsFactors = FALSE
+ )
> 
> # set outcome variable, classe as a factor
> raw$classe <- factor(raw$classe)
> 
> # order by window num and raw_timestamp_part_2 to mimic a timeseries
> raw <- arrange(raw, num_window, raw_timestamp_part_2)
> 
> # split into train (70%) and test (30%) on classe
> set.seed(033)
> rawindex <-
+     createDataPartition(raw$classe, p = 0.7, list = FALSE, times = 1)
> training <- raw[rawindex,]
> testing <- raw[-rawindex,]
> 
> # density plot - shows how exercise classe are represented
> png(filename = "data/plot-classe-density.png", width = 640, height = 480, units = "px")
> qplot(classe, data = training, geom = "density", colour = classe)
> dev.off()
null device 
          1 
> 
> # do we have a good classe split?
> # table(training$classe)
> # table(testing$classe)
> 
> # do we have any window overlap between train and test?
> # intersect(unique(training$num_window), unique(testing$num_window))
> 
> # prepare model formula:
> 
> # ignore columns that are more than 95% empty (i.e. NA):
> nasPerc <- as.integer(0.95 * nrow(raw))
> nas <- sort(apply(raw, 2, function(x) length(which(is.na(x)))), decreasing = TRUE)
> badNames <- names(nas[nas >= nasPerc])
> # print(badNames)
> goodNames <- setdiff(names(training), badNames)
> # print(goodNames)
> # exclude columns that do not aid in prediction (or are an outcome)
> # method: (inner to outer)
> # - paste together with OR condition all names to exclude
> # - grep returning inverted matched names (i.e. collect those that don't match)
> # - sort alphabetically to help us humans
> trainNames <- sort(grep(
+     paste("classe", "_window", "user_name", "X", "_timestamp", sep = "|"),
+     goodNames, value = TRUE, invert = TRUE)
+ )
> # print(trainNames)
> # use these names to generate training formula
> trainFormula <- as.formula(paste("classe ~ ", paste(trainNames, collapse = "+")))
> print(trainFormula)
classe ~ accel_arm_x + accel_arm_y + accel_arm_z + accel_belt_x + 
    accel_belt_y + accel_belt_z + accel_dumbbell_x + accel_dumbbell_y + 
    accel_dumbbell_z + accel_forearm_x + accel_forearm_y + accel_forearm_z + 
    gyros_arm_x + gyros_arm_y + gyros_arm_z + gyros_belt_x + 
    gyros_belt_y + gyros_belt_z + gyros_dumbbell_x + gyros_dumbbell_y + 
    gyros_dumbbell_z + gyros_forearm_x + gyros_forearm_y + gyros_forearm_z + 
    magnet_arm_x + magnet_arm_y + magnet_arm_z + magnet_belt_x + 
    magnet_belt_y + magnet_belt_z + magnet_dumbbell_x + magnet_dumbbell_y + 
    magnet_dumbbell_z + magnet_forearm_x + magnet_forearm_y + 
    magnet_forearm_z + pitch_arm + pitch_belt + pitch_dumbbell + 
    pitch_forearm + roll_arm + roll_belt + roll_dumbbell + roll_forearm + 
    total_accel_arm + total_accel_belt + total_accel_dumbbell + 
    total_accel_forearm + yaw_arm + yaw_belt + yaw_dumbbell + 
    yaw_forearm
> 
> # model using random forest
> if (file.exists("data/model-lda.rds")) {
+     print("Restoring model ...")
+     model <- readRDS("data/model-lda.rds")
+ } else {
+     print("Building model ...")
+     # record start time of model build
+     starttime <- proc.time()
+     model <- train(trainFormula, data = training, method = "lda")
+     # how long did this model take to build?
+     print(paste("Total elapsed time is:", (proc.time() - starttime)[["elapsed"]], "secs"))
+     # save model
+     saveRDS(model, "data/model-lda")
+ }
[1] "Restoring model ..."
> 
> # print some information on this model
> print(model$finalModel$problemType)
[1] "Classification"
> print(model$method)
[1] "lda"
> print(model$finalModel$xNames)
 [1] "accel_arm_x"          "accel_arm_y"          "accel_arm_z"         
 [4] "accel_belt_x"         "accel_belt_y"         "accel_belt_z"        
 [7] "accel_dumbbell_x"     "accel_dumbbell_y"     "accel_dumbbell_z"    
[10] "accel_forearm_x"      "accel_forearm_y"      "accel_forearm_z"     
[13] "gyros_arm_x"          "gyros_arm_y"          "gyros_arm_z"         
[16] "gyros_belt_x"         "gyros_belt_y"         "gyros_belt_z"        
[19] "gyros_dumbbell_x"     "gyros_dumbbell_y"     "gyros_dumbbell_z"    
[22] "gyros_forearm_x"      "gyros_forearm_y"      "gyros_forearm_z"     
[25] "magnet_arm_x"         "magnet_arm_y"         "magnet_arm_z"        
[28] "magnet_belt_x"        "magnet_belt_y"        "magnet_belt_z"       
[31] "magnet_dumbbell_x"    "magnet_dumbbell_y"    "magnet_dumbbell_z"   
[34] "magnet_forearm_x"     "magnet_forearm_y"     "magnet_forearm_z"    
[37] "pitch_arm"            "pitch_belt"           "pitch_dumbbell"      
[40] "pitch_forearm"        "roll_arm"             "roll_belt"           
[43] "roll_dumbbell"        "roll_forearm"         "total_accel_arm"     
[46] "total_accel_belt"     "total_accel_dumbbell" "total_accel_forearm" 
[49] "yaw_arm"              "yaw_belt"             "yaw_dumbbell"        
[52] "yaw_forearm"         
> print(model$finalModel$obsLevels)
[1] "A" "B" "C" "D" "E"
> print(model$finalModel$confusion)
NULL
> print(model$metric)
[1] "Accuracy"
> 
> # test
> testPredict <- predict(model, newdata = testing)
> # estimate error (since this is categorical data we are estimating accuracy)
> errorRate <- function(trueValues, predictValues) {
+     sum(trueValues != predictValues) / length(trueValues)
+ }
> errorRate(testing$classe, testPredict)
[1] 0.2934579
> 
> # print Accuracy and Kappa (measure of rating variable(s) agreement)
> postResample(testPredict, testing$classe)
 Accuracy     Kappa 
0.7065421 0.6284848 
> 
> # show confusion matrix
> confusionMatrix(data = testPredict, reference = testing$classe)
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1376  179  104   53   43
         B   43  733  107   37  197
         C  124  133  679  119   86
         D  124   40  107  721  107
         E    7   54   29   34  649

Overall Statistics
                                          
               Accuracy : 0.7065          
                 95% CI : (0.6947, 0.7182)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.6285          
 Mcnemar's Test P-Value : < 2.2e-16       

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.8220   0.6435   0.6618   0.7479   0.5998
Specificity            0.9100   0.9191   0.9049   0.9232   0.9742
Pos Pred Value         0.7840   0.6562   0.5951   0.6561   0.8396
Neg Pred Value         0.9278   0.9148   0.9269   0.9492   0.9153
Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
Detection Rate         0.2338   0.1246   0.1154   0.1225   0.1103
Detection Prevalence   0.2982   0.1898   0.1939   0.1867   0.1314
Balanced Accuracy      0.8660   0.7813   0.7834   0.8356   0.7870
> 
> # validate against project test data
> 
> # The final model was run against validation data:
> validation <- read.csv(
+     "data/pml-testing.csv", header = TRUE,
+     na.strings = c("NA", "#DIV/0!"), stringsAsFactors = FALSE
+ )
> validationPredict <- predict(model, newdata = validation)
> # save answers
> saveRDS(validationPredict, "data/predictions-lda.rds")
> 
> # test predictions against known results
> rfPred <- readRDS("data/predictions-rf.rds")
> ldaPred <- readRDS("data/predictions-lda.rds")
> print(rfPred)
 [1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
> print(ldaPred)
 [1] B A B C C E D D A A D A B A E A A B B B
Levels: A B C D E
> rfPred == ldaPred
 [1]  TRUE  TRUE  TRUE FALSE FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE
[13]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE
> errorRate(rfPred, ldaPred)
[1] 0.3
> 
