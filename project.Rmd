---
title: "Predict Correct Use of Dumbbells"
author: "Frank Jung"
date: "20 October 2015"
output:
  html_document:
    fig_caption: yes
    highlight: monochrome
    toc: yes
---

```{r initialisation, echo=FALSE, message=FALSE, warning=FALSE}
require(knitr, quietly = TRUE)
require(randomForest, quietly = TRUE)
require(dplyr, quietly = TRUE)
require(ggplot2, quietly = TRUE)
require(rfUtilities, quietly = TRUE)
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

```{r load, echo=FALSE}
# load raw data
raw <- read.csv("data/pml-training.csv", header = TRUE, na.strings = c("NA", "#DIV/0!"), stringsAsFactors = FALSE)

# set classe as a factor
raw$classe <- factor(raw$classe)
```

## Overview

This report describes the algorithm used to predict how well people perform the
exercise of lifting a dumbbell. This is a classification problem where
predictors are used to determine one of five [outcomes](#goal). This report
will:

* [describe the choices made in choosing a model](#choice-of-prediction-algorithm)
* [describe how the model was built](#data-partitioning)
* [cross validate the model](#testing)
* [estimate the out-of-sample error rate](#estimation-of-error-rate)

Links to data and further information of the original research can be found in
the [appendices](#references).

### Background

Using devices such as [Jawbone Up](https://jawbone.com/up), [Nike
FuelBand](http://www.nike.com/), and [Fitbit](https://www.fitbit.com) it is now
possible to collect a large amount of data about personal activity relatively
inexpensively. These type of devices are part of the quantified self movement,
a group of enthusiasts who take measurements about themselves regularly to
improve their health, to find patterns in their behavior, or because they are
tech geeks. One thing that people regularly do is quantify how much of a
particular activity they do, but they rarely quantify how well they do it.

### Goal

Here we wish to predict the manner in which people do an exercise from data
collected from accelerometers on the belt, forearm, arm, and dumbbell of 6
participants. They were asked to perform dumbbell lifts correctly and
incorrectly in 5 different ways:

```{r classe-table}
classeId <- c(levels(raw$classe))
classeDesc <- c("exactly according to the specification",
                 "throwing the elbows to the front",
                 "lifting the dumbbell only halfway",
                 "lowering the dumbbell only halfway",
                 "and throwing the hips to the front")
classeTable <- data.frame(Classe = classeId, Description = classeDesc)
kable(classeTable, caption = "Classes of exercise activity")
```

The prediction algorithm will determine which classification an exercise falls
into.

## Exploratory Analysis

The ``classe`` field is a categorical outcome, which we will treat as a factor.

The data contains a large number (`r ncol(raw)`) of columns. However, many of
these columns contain little or no information or contain an Excel ``#DIV/0!``
conversion error. Consequently, we will exclude from the prediction formula
all columns with a 95% or higher of missing data.

A number of columns will also be excluded since they do not provide value to
prediction. They are:

| Column | Reason |
|--------|--------|
| ``X`` | Row index not related to a measurement |
| ``*_window``, ``*_timestamp*`` | measurements were made in time segments with overlap so they can be treated as discrete and not time dependent |
| ``user_name`` | who performed the action is not relevant as all were supervised to ensure consistent actions |

Table: Additional columns to ignore

## Training

### Choice of Prediction Algorithm

A good performing prediction algorithm for classification problems is [Random
Forest](https://en.wikipedia.org/wiki/Random_forest). It is resilient to
outliers but is affected by multi-collinearity. This will [be
tested](#check-multi-collinearity) later.

The predictors used in the formula, were tested using
[rfUtilities](http://cran.r-project.org/package=rfUtilities).

### Data Partitioning

The [training data](#data) was partitioned into two:

* training (70%) - used to train model
* testing (30%) - used for [cross-validation](#testing), and estimate the [error rate](#estimation-of-error-rate)

```{r partition, echo = TRUE}
# split into train (70%) and test (30%)
set.seed(033)
dataSize <- nrow(raw)
sampleSize <- floor(0.70 * dataSize)
rawindex <- sample(seq_len(dataSize), size = sampleSize)
training <- raw[rawindex,]
testing <- raw[-rawindex,]
```

### Training Formula

The model prediction formula is composed of all fields, **except**:

* columns that are more than 95% empty
* non-predictive columns identified in [Table: Additional columns to ignore](#exploratory-analysis)

Applying these conditions gives the prediction formula:

```{r formula, echo = TRUE}
# ignore columns that are more than 95% empty (i.e. NA):
nasPerc <- as.integer(0.95 * nrow(raw))
nas <- sort(apply(raw, 2, function(x) length(which(is.na(x)))), decreasing = TRUE)
badNames <- sort(names(nas[nas >= nasPerc]))
goodNames <- setdiff(names(training), badNames)

# exclude columns that do not aid in prediction (or are an outcome)
trainNames <-
    grep(
      paste("classe", "window", "user_name", "X", "_timestamp", sep = "|"),
      goodNames, value = TRUE, invert = TRUE
    )

# use these column names to generate training formula
trainFormula <- as.formula(paste("classe ~ ", paste(trainNames, collapse = "+")))
print(trainFormula)
```

This will appear below in training as the variable ``trainFormula``.

### Training Model

The Random Forest was trained using the 
[randomForest](https://www.stat.berkeley.edu/~breiman/RandomForests/) package
with the defaults:

```r
model <- randomForest(trainFormula, data = training)
```

```{r train}
# model using random forest
if (file.exists("data/model-rf.rds")) {
    model <- readRDS("data/model-rf.rds")
}
```

### Check Multi-Collinearity

No collinear variables were identified. The model was tested with:

```{r collinear, echo = TRUE}
multi.collinear(dplyr::select(training, one_of(trainNames)))
```

### Variable Importance

A plot of [variable
importance](https://en.wikipedia.org/wiki/Random_forest#Variable_importance)
using Mean Decrease
[Gini](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity):

```{r importancePlot}
imp <- importance(model)

# put into data frame for use in plot and table
impVar <- data.frame(Index = 1:nrow(imp), Variable = rownames(imp), Importance = imp[, 1])

# plot
ggplot(data = impVar, aes(x = Index, y = Importance)) +
    geom_point(shape = 1) +
    geom_line(colour = "purple") +
    theme_light(base_family = "sans", base_size = 11) +
    scale_y_continuous(breaks = seq(0, 900, by = 50)) +
    scale_x_discrete(breaks = seq(0, 52, by = 2)) +
    labs(y = "Importance (Mean Decrease Gini)") +
    ggtitle("Variable Importance")
```

The full list by variable importance:

```{r importanceTable}
# full table in decreasing order of importance
kable(arrange(impVar, desc(Importance)), caption = "Variable Importance")
```

## Testing

The model was next cross-validated against the reserved test data:

```{r predict, echo = TRUE}
# cross-validate
testPredict <- predict(model, newdata = testing)
```

### Estimation of Error Rate

Since this is categorical data, estimate using an accuracy measure:

```{r estimateErrorRate, echo = TRUE}
# estimate error (since this is categorical data we are estimating accuracy)
errorRate <- function(trueValues, predictValues) {
    sum(trueValues != predictValues) / length(trueValues)
}
estimated <- errorRate(testing$classe, testPredict)
```

The estimated error rate for this model is **`r round(100 * estimated, 3)`%**.

### Accuracy and Cohen Kappa

The accuracy and Cohen [Kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa)
statistics are:

```{r accuracy}
require(caret, quietly = TRUE)
cm <- confusionMatrix(data = testPredict, reference = testing$classe)
# ignore AccuracyNull(5) and McnemarPValue(7)
overallStats <- data.frame(Value = round(cm$overall[-c(5,7)], 3))
kable(x = overallStats, caption = "Overall Statistics")
```

### Confusion Matrix

The prediction [confusion
matrix](https://en.wikipedia.org/wiki/Confusion_matrix) is:

```{r confusionMatrix}
kable(x = cm$table, caption = "Confusion Matrix")
```

### Statistics by Class

Statistics by class:

```{r statistics}
byClass <- round(cm$byClass, 3)
kable(t(byClass), caption = "Statistics by Class")
```

## Appendices

### References

Source research paper:

* Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013. [PDF](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf)

* [A Brief Tour of the Trees and Forests](http://www.r-bloggers.com/a-brief-tour-of-the-trees-and-forests/)

### Data

Data for this project was sourced from:

* Training data, https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv
* Validation data, https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

### Source

The source code and model data for this report are available from GitHub,
https://github.com/frankhjung/predmachlearn-033.

| Name | Description |
|------|-------------|
| getdata.R | download raw CSV data from remote sources |
| model-rf.R | full script to train and test random forest model |
| pred.R | prepare predictions for submission |

### Session Information

This report was produced using the following RStudio environment:

```{r session}
sessionInfo()
```
